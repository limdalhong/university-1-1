{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 강화학습을 이용한 오델로(Othello)게임 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2019102083 김연수<br><br>2019102123 임달홍\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 오델로(Othello)란?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오델로(Othello)는 보드 게임의 한 종류이다. 리버시(Reversi)라고도 불립니다. <br><br>두 명이 가로 세로 8칸의 오델로 판 위에서 한쪽은 검은색, 다른 한쪽은 흰색인 돌을 번갈아 놓으며 진행됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![오델로](https://www.eothello.com/images/how_to_play_othello_0.png \"othello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <게임 규칙>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 처음에 판 가운데에 사각형으로 엇갈리게 배치된 돌 4개를 놓고 시작한다.<br><br>\n",
    "* 돌은 반드시 상대방 돌을 양쪽에서 포위하여 뒤집을 수 있는 곳에 놓아야 한다.<br><br>\n",
    "* 돌을 뒤집을 곳이 없는 경우에는 차례가 자동적으로 상대방에게 넘어가게 된다.<br><br>\n",
    "* 아래와 같은 조건에 의해 양쪽 모두 더 이상 돌을 놓을 수 없게 되면 게임이 끝나게 된다.\n",
    "> 64개의 돌 모두가 판에 가득 찬 경우 (가장 일반적)<br><br>\n",
    "> 어느 한 쪽이 돌을 모두 뒤집은 경우<br><br>\n",
    "> 한 차례에 양 쪽 모두 서로 차례를 넘겨야 하는 경우\n",
    "\n",
    "* 게임이 끝났을 때 돌이 많이 있는 플레이어가 승자가 된다. 만일 돌의 개수가 같을 경우는 무승부가 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "출처 : https://ko.wikipedia.org/wiki/%EC%98%A4%EB%8D%B8%EB%A1%9C\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 주제 선정 이유"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오델로는 2인 제로섬 유한 확정 완전정보 게임으로, <br><br>서로가 최선의 수를 둘 경우 반드시 선수필승 혹은 후수필승 혹은 무승부의 결과가 나오며, 이론상 상대의 모든 수를 미리 읽을 수 있는 게임입니다.<br><br> 오델로를 제외하면 장기, 바둑, 오목 등이 이에 해당됩니다.<br><br>따라서, 이론상으로는 오델로도 필승법을 알아낼 수 있는 게임에 해당합니다.<br><br>하지만 대부분의 보드게임은 경우의 수가 매우 많고, 인간의 머리로는 모든 수를 읽는 것이 불가능하기에 <br><br>강화학습(Reinforcement Learning)을 활용해 최선의 수가 무엇인지 탐색해보고자 합니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 가설 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오델로 게임의 특성상, 현재 상대의 돌을 얼마나 많이 뒤집느냐(당장의 이득)는 크게 중요하지 않은 경우가 많다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://greenothello.com/content/library/ffo_guide/3_1.jpg\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그림 1의 네칸 남은 상황에서 흑은 단지 하나의 돌만을 가지고 있다. 그럼 백이 이기는 걸까?<br><br>\n",
    "흑은 a1이나 h8 둘중에 한곳에 둘 수 있다.<br><br>\n",
    "그리고 이 상황에서는 남은 칸을 모두 흑이 두게 되고(백은 둘 곳이 없기 때문에 흑의 매 수 다음에 패스가 난다),<br><br>\n",
    "최종 스코어는 40-24로 흑의 승리가 된다.<br><br>\n",
    "여기에서 게임이 거의 끝나갈 때 많은 돌을 가지고 있는 건 이기는데 큰 도움이 안된다는 것을 알 수 있다.<br><br>\n",
    "이에 따라, 당장의 수가 뒤집는 돌의 개수(얻는 현재의 이득)보다는<br><br>\n",
    "몇 수를 더 두었을 때의 수의 유리함(미래의 이득)을 예측할 수 있어야 한다는 이야기가 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "출처 : https://greenothello.com/library/ffo_guide3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "따라서, 강화학습을 통해 수를 놓았을 때 미래에 얻을 수 있는 이득의 기댓값을 예측할 수 있게 된다면\n",
    "\n",
    "> 강화학습을 통해 오델로 게임의 승률을 비약적으로 끌어올리거나 필승법을 알아낼 수 있지 않을까? \n",
    "\n",
    "라는 가설을 세울 수 있다.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 방향성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 오델로의 게임 규칙에 맞게 게임 설계 및 제작<br><br>\n",
    "* 오델로 게임을 플레이하고 학습할 Agent 구현<br><br>\n",
    "* 학습 진행 및 결과 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 오델로 게임 제작"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 제시했던 오델로 게임의 규칙에 맞게 오델로 게임을 제작했습니다.<br><br>\n",
    "단, 이후에 플레이 내용을 저장하고 학습할 수 있도록 구성해야한다는 점을 유념해야 했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from colorama import init, Fore, Back, Style\n",
    "init(autoreset=True)\n",
    "\n",
    "\n",
    "class Board(object):\n",
    "    BLACK = 1\n",
    "    WHITE = -1\n",
    "\n",
    "    def __init__(self):\n",
    "        self.board = np.zeros((8,8), int)\n",
    "        self.board[3][3] = Board.BLACK\n",
    "        self.board[4][4] = Board.BLACK\n",
    "        self.board[4][3] = Board.WHITE\n",
    "        self.board[3][4] = Board.WHITE\n",
    "\n",
    "        self.remaining_squares = 8*8 - 4\n",
    "        self.score = {Board.BLACK: 2, Board.WHITE: 2}\n",
    "\n",
    "    def getScore(self):\n",
    "        return self.score\n",
    "\n",
    "    def getState(self):\n",
    "        return self.board\n",
    "\n",
    "    def isOnBoard(self, x, y):\n",
    "        return x >= 0 and x <= 7 and y >= 0 and y <= 7\n",
    "\n",
    "    def updateBoard(self, tile, row, col):\n",
    "        result = self.isValidMove(tile, row, col)\n",
    "        if result:\n",
    "            self.board[row][col] = tile\n",
    "            for row in result:\n",
    "                self.board[row[0]][row[1]] = tile\n",
    "\n",
    "            self.score[tile] += len(result) + 1\n",
    "\n",
    "            self.score[(((tile+1)//2+1)%2)*2-1] -= len(result)\n",
    "\n",
    "            self.remaining_squares -= 1\n",
    "\n",
    "            return True\n",
    "\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def isValidMove(self, tile, xstart, ystart):\n",
    "        if not self.isOnBoard(xstart, ystart) or self.board[xstart][ystart] != 0:\n",
    "            return False\n",
    "        self.board[xstart][ystart] = tile\n",
    "\n",
    "        otherTile = tile * -1\n",
    "\n",
    "        tiles_to_flip = []\n",
    "        for xdirection, ydirection in ((0,1),(1,1),(1,0),(1,-1),(0,-1),(-1,-1),(-1,0),(-1,1)):\n",
    "            x, y = xstart, ystart\n",
    "            x += xdirection \n",
    "            y += ydirection \n",
    "            if self.isOnBoard(x, y) and self.board[x][y] == otherTile:\n",
    "                x += xdirection\n",
    "                y += ydirection\n",
    "                if not self.isOnBoard(x, y):\n",
    "                    continue\n",
    "                while self.board[x][y] == otherTile:\n",
    "                    x += xdirection\n",
    "                    y += ydirection\n",
    "                    if not self.isOnBoard(x, y):\n",
    "                        break\n",
    "                if not self.isOnBoard(x, y):\n",
    "                    continue\n",
    "                if self.board[x][y] == tile:\n",
    "                    while True:\n",
    "                        x -= xdirection\n",
    "                        y -= ydirection\n",
    "                        if x == xstart and y == ystart:\n",
    "                            break\n",
    "                        tiles_to_flip.append([x, y])\n",
    "\n",
    "        self.board[xstart][ystart] = 0\n",
    "\n",
    "        return tiles_to_flip\n",
    "\n",
    "    def printBoard(self):\n",
    "\n",
    "        def getItem(item):\n",
    "            if item == Board.BLACK :\n",
    "                return Fore.WHITE + \"|\" + Fore.BLACK + \"O\"\n",
    "            elif item == Board.WHITE :\n",
    "                return Fore.WHITE + \"|\" + Fore.WHITE + \"O\"\n",
    "            else:\n",
    "                return Fore.WHITE + \"| \"\n",
    "\n",
    "        def getRow(row):\n",
    "            return \"\".join(map(getItem,row))\n",
    "\n",
    "        print(\"\\t\" + Back.GREEN +              \"      BOARD      \")\n",
    "        print(\"\\t\" + Back.GREEN + Fore.WHITE + \" |0|1|2|3|4|5|6|7\")\n",
    "        for i in range(8):\n",
    "            print(\"\\t\" + Back.GREEN + Fore.WHITE + \"{}{}\".format(i,\n",
    "                getRow(self.board[i])))\n",
    "            sys.stdout.write(Style.RESET_ALL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "게임판 위에 있는 돌들의 초기 위치를 설정한 후,<br><br> 게임판 위에 놓이게 될 돌(input)들이 유효한 수인지 검사하는 board.py를 작성합니다. <br><br> debug 과정에서 colorama 모듈을 통해 보드가 잘 만들어졌는지를 확인하는 시각화까지 진행했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import board\n",
    "\n",
    "\n",
    "class Game:\n",
    "    def __init__(self):\n",
    "        self.players = []\n",
    "        self.board = board.Board()\n",
    "\n",
    "    def addPlayer(self, player, log_move_history = True):\n",
    "        self.players.append((player, log_move_history))\n",
    "\n",
    "    def getScore(self):\n",
    "        return self.board.getScore()\n",
    "\n",
    "    def run(self, show_board = False):\n",
    "        n_passed = 0\n",
    "        \n",
    "        while n_passed < 2:\n",
    "            n_passed = 0\n",
    "            for i, player in enumerate(self.players):\n",
    "                pid = i*2-1\n",
    "                did_move = player[0].play(lambda r,c: self.board.updateBoard(pid,r,c),\n",
    "                                   self.board.getState(), pid, player[1])\n",
    "\n",
    "                if show_board:\n",
    "                    self.board.printBoard()\n",
    "\n",
    "                if not did_move:\n",
    "                    n_passed += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "플레이어를 추가하고 게임을 진행시키는 game.py를 작성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import board\n",
    "\n",
    "class HumanPlayer:\n",
    "    def play(self, place_func, board_state, me, _):\n",
    "        try:\n",
    "            pos = map(int, map(str.strip, input().split(\" \")))\n",
    "            place_func(*pos)\n",
    "            return True\n",
    "        except ValueError:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우선 게임이 규칙이 맞게 잘 실행되는지를 확인하기 위해 게임을 플레이할 수 있는 사람을 플레이어로 지정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from game import Game\n",
    "from players import *\n",
    "\n",
    "h1 = HumanPlayer()\n",
    "h2 = HumanPlayer()\n",
    "\n",
    "g = Game()\n",
    "g.addPlayer(h1)\n",
    "g.addPlayer(h2)\n",
    "g.run(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그리고 게임을 실행시켜봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![게임 실행 모습](./gamevisual.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "입력한 좌표에 돌이 놓이며 게임이 잘 실행되는 것을 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 컴퓨터 플레이어 제작 및 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from scipy.special import expit\n",
    "\n",
    "\n",
    "class NN:\n",
    "    def __init__(self, layer_dims, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.layer_dims = layer_dims\n",
    "        self.layers = []\n",
    "        for i in range(len(layer_dims)-1):\n",
    "            self.layers.append(np.random.normal(0, 1, size=(layer_dims[i+1], layer_dims[i]+1)))\n",
    "\n",
    "        self.activation_func  = None\n",
    "        self.dactivation_func = None\n",
    "\n",
    "    def save(self, filename):\n",
    "        with open(filename, \"wb\") as f:\n",
    "            pickle.dump(self.layers, f)\n",
    "\n",
    "    def load(self, filename):\n",
    "        with open(filename, \"rb\") as f:\n",
    "            self.layers = pickle.load(f)\n",
    "\n",
    "    def mkVec(self, vector1D, add_bias = True):\n",
    "        return np.reshape(vector1D, (len(vector1D), 1))\n",
    "\n",
    "    def getOutput(self, input_vector):\n",
    "        outputs = input_vector\n",
    "        for i in range(len(self.layers)):\n",
    "            outputs = activation(self.layers[i]@np.vstack((outputs, 1)))\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def backProp(self, sample, target):\n",
    "        outputs = [sample]\n",
    "        for i in range(len(self.layers)):\n",
    "            outputs.append(activation(self.layers[i].dot(np.vstack((outputs[i], 1)))))\n",
    "\n",
    "\n",
    "        layer_deltas = np.empty(len(outputs) - 1, object)\n",
    "\n",
    "        layer_deltas[-1] = (target - outputs[-1]) * dactivation(outputs[-1])\n",
    "        for i in range(len(layer_deltas) - 2, -1, -1):\n",
    "            layer_derivative = dactivation(outputs[i+1])\n",
    "\n",
    "            layer_deltas[i] = layer_derivative * (self.layers[i+1].T.dot(layer_deltas[i + 1])[:-1])\n",
    "\n",
    "        for i in range(len(self.layers)):\n",
    "            self.layers[i] += self.learning_rate * np.c_[outputs[i].T, 1] * layer_deltas[i]\n",
    "\n",
    "        return outputs[-1]\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return np.multiply(x > 0, x)\n",
    "\n",
    "def drelu(x):\n",
    "    return np.float64(x > 0)\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    e = np.exp(x)\n",
    "    return e/np.sum(e)\n",
    "\n",
    "\n",
    "def activation(x):\n",
    "\n",
    "    return np.tanh(x)\n",
    "\n",
    "def dactivation(x):\n",
    "    return 1 - np.tanh(x)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLPlayer:\n",
    "    def __init__(self, q_lr, discount_factor, net_lr = 0.01):\n",
    "        self.policy_net = nn.NN([64, 128, 128, 64, 64], net_lr)\n",
    "\n",
    "        self.epsilon = 0.6\n",
    "\n",
    "        self.q_lr = q_lr\n",
    "        self.discount_factor = discount_factor\n",
    "        self.play_history = []\n",
    "        self.wins = 0\n",
    "\n",
    "    def play(self, place_func, board_state, me, log_history = True):\n",
    "        input_state = np.apply_along_axis(lambda x: int((x==me and 1) or (x!=0 and -1)),\n",
    "                                          1, board_state.reshape((64, 1))).reshape((64,1))\n",
    "        made_move = False\n",
    "        pos = None\n",
    "\n",
    "        if np.random.random() < self.epsilon:\n",
    "            positions = list(itertools.product(range(8), repeat = 2))\n",
    "            random.shuffle(positions)\n",
    "            while not made_move and positions:\n",
    "                pos = positions.pop()\n",
    "                made_move = place_func(*pos)\n",
    "\n",
    "            if not made_move and not positions:\n",
    "                return False\n",
    "\n",
    "        else:\n",
    "            out = self.policy_net.getOutput(input_state)\n",
    "            positions = [(v,i) for i,v in enumerate(out)]\n",
    "            positions.sort(key = lambda x: x[0], reverse = True)\n",
    "\n",
    "            while not made_move and positions:\n",
    "                scalar_play_point = positions.pop()[1]\n",
    "                pos = scalar_play_point // 8, scalar_play_point % 8\n",
    "                made_move = place_func(*pos)\n",
    "\n",
    "            if not made_move and not positions:\n",
    "                return False\n",
    "\n",
    "        if log_history:\n",
    "            self.play_history.append((np.copy(input_state), pos[0]*8 + pos[1]))\n",
    "\n",
    "        return True\n",
    "\n",
    "    def updateWeights(self, final_score):\n",
    "        i = 0\n",
    "        state, action = self.play_history[i]\n",
    "        q = self.policy_net.getOutput(state)\n",
    "        n_play_history = len(self.play_history)\n",
    "        while i < n_play_history:\n",
    "            i += 1\n",
    "            if i == n_play_history:\n",
    "                q[action] = final_score\n",
    "\n",
    "            else:\n",
    "                state_, action_ = self.play_history[i]\n",
    "                q_ = self.policy_net.getOutput(state_)\n",
    "                q[action] += self.discount_factor * np.max(q_)\n",
    "\n",
    "            self.policy_net.backProp(state, self.policy_net.mkVec(q))\n",
    "\n",
    "            if i != n_play_history:\n",
    "                action, q = action_, q_\n",
    "\n",
    "\n",
    "def OneHot(index, dim):\n",
    "    a = np.zeros((dim,1))\n",
    "    a[index] = 1\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q Learning을 기반으로 한 컴퓨터 플레이어 class를 만들고,<br><br> 컴퓨터 둘이 서로 othello 게임을 플레이하게 만든 뒤 이 결과를 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from game import Game\n",
    "from players import RLPlayer\n",
    "\n",
    "player = RLPlayer(0.07, 0.99, 0.03)\n",
    "rp = RLPlayer(0, 0)\n",
    "\n",
    "match_size = 10\n",
    "n_epochs = 2000\n",
    "\n",
    "player_wins = []\n",
    "for e in range(n_epochs):\n",
    "    print(\"Epoch: %d\"%e)\n",
    "\n",
    "    player.wins = 0\n",
    "    player.epsilon = (np.exp(-0.017*e)+0.11)/1.1\n",
    "    player_gameplay_history = []\n",
    "\n",
    "    for _ in range(match_size):\n",
    "        print(\"Game: %d\"%_)\n",
    "        player.play_history = []\n",
    "\n",
    "        g = Game()\n",
    "        g.addPlayer(player)\n",
    "        g.addPlayer(rp, False)\n",
    "        g.run()\n",
    "\n",
    "        final_score = list(g.getScore().items())\n",
    "        final_score.sort()\n",
    "        ttl = sum(map(lambda x: x[1], final_score))\n",
    "        player_score =  (final_score[0][1]/ttl - 0.5)*2\n",
    "        player.wins += player_score > 0\n",
    "        print(player_score)\n",
    "        player_gameplay_history.append((player.play_history, player_score))\n",
    "\n",
    "    print(player.epsilon, player.wins)\n",
    "    player_wins.append(player.wins)\n",
    "    for game, score in player_gameplay_history:\n",
    "        player.play_history = game\n",
    "        player.updateWeights(score)\n",
    "\n",
    "suffix = \"linear-0.03\"\n",
    "player.policy_net.save(\"best-%s.weights\"%suffix)\n",
    "print(sum(player_wins))\n",
    "with open(\"%d-%d-%s.csv\"%(n_epochs, match_size, suffix), \"w\") as f:\n",
    "    f.write(\"\\n\".join(map(str, player_wins)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한 세트에 10판씩, 총 2000세트를 진행해서 학습시키고 이 결과를 저장합니다.<br><br>\n",
    "각 세트의 승률은 \"2000-10-linear-0.03.csv\"로, <br><br>\n",
    "학습 결과는 \"best-linear-0.03.weights\"로 저장됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 결론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from game import Game\n",
    "from players import *\n",
    "\n",
    "h = HumanPlayer()\n",
    "ai = RLPlayer(1,1,1)\n",
    "ai.policy_net.load(\"best-linear-0.03.weights\")\n",
    "\n",
    "g = Game()\n",
    "g.addPlayer(h)\n",
    "g.addPlayer(ai)\n",
    "g.run(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "저장된 \"best-linear-0.03.weights\"대로 수를 두는 컴퓨터 플레이어와 사람이 플레이를 해 보았습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](hwins.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 과정을 기록한 csv파일에서도 알 수 있었지만, 학습을 거듭한다고 해서 먼저 두는 쪽의 승률이 비약적으로 상승하지는 않았습니다.<br><br> 위 사진은 AI가 흑돌로 선수를 두었지만 사람에게 패배한 경우였습니다.<br><br> AI의 특징은 유리한 수가 어디인지는 아는 것 같지만 결정적으로 승리를 가져갈수 있는 위치에 돌을 두는 방법을 잘 모르는 듯하다는 것이었습니다.<br><br>결론적으로 20000번정도의 게임 학습으로는 승률 100%의 모델을 만들 수 없었을 뿐만 아니라, <br><br>이와 같은 양상을 보아 학습을 더 진행하게 되었을 때 컴퓨터가 더 유리하게 게임을 이끌어나갈 수 있는 능력을 가질 수 있을지도 의문이었습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
